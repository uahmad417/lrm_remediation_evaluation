{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ead90c61-6d05-4745-975c-dc61d16e4f3d",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb70618-c260-4633-9e43-1de609305f60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install langchain langchain_core langchain_community langchain-huggingface torch accelerate bitsandbytes docarray unstructured jq openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b74d267-a2ea-48b5-b2d3-cf5df55d918a",
   "metadata": {},
   "source": [
    "## Environment Variables and Constants\n",
    "\n",
    "Set the API keys and environment variables required for running the app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33efd142-6c55-40ea-b734-e99f11085006",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"\"\n",
    "os.environ['LANGCHAIN_API_KEY'] = \"\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "\n",
    "# this defines the location where the models will be downloaded\n",
    "# it is suggested to set this to a location with sufficient storage\n",
    "# space. Not specifying it will default to `${HOME}/.cache/hugging_face`\n",
    "os.environ[\"HF_HUB_CACHE\"] = \"\"\n",
    "os.environ[\"HF_HOME\"] = \"\"\n",
    "TEMPLATE = \"\"\"\n",
    "You are an assistant in security risk analysis.\n",
    "You will be provided with risk scenarios that have certain threats and vulnerabilities. For the threats you will also be provided with possible counter measures.\n",
    "You will be provided with a user scenario and based on that you will be provided with context of related scenarios from you retrieval vector store.\n",
    "You will also be provided with possible countermeasures for all the retrieved scenarios.\n",
    "You need to suggest the appropriate countermeasures for the user scenario and give reasoning as to why it is appropriate. There could be multiple appropriate countermeasures so you need to provide a reasoning for each individually.\n",
    "Answer the question based only on the following context. If the question does not relate with the context, just reply 'I don't know'\n",
    "\n",
    "User: {user}\n",
    "\n",
    "Scenarios: {scenarios}\n",
    "\n",
    "Countermeasures: {countermeasures}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c92f463-8c24-46cd-9b32-ca5a098efd6b",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "Install the requirements and import relevant modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7120a69-f4f9-427c-a109-174941a2ec8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from json import JSONDecodeError\n",
    "from typing import Any, List\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from langchain_community.document_loaders import (JSONLoader,\n",
    "                                                  UnstructuredExcelLoader)\n",
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "from langchain_core.documents.base import Document\n",
    "from langchain_core.exceptions import OutputParserException\n",
    "from langchain_core.output_parsers import JsonOutputParser, StrOutputParser\n",
    "from langchain_core.outputs import Generation\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.runnables.base import RunnableLambda, RunnableSequence\n",
    "from langchain_huggingface import (ChatHuggingFace, HuggingFaceEmbeddings,\n",
    "                                   HuggingFacePipeline)\n",
    "from pandas import DataFrame\n",
    "from transformers import BitsAndBytesConfig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58faceb8-cf6a-4da4-8510-b33d55bdebf9",
   "metadata": {},
   "source": [
    "## Data Loaders\n",
    "\n",
    "Methods used for loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3479ee1-2c7c-439a-85e4-037a4a2518a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_remediations_file(file_path: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Loads the remediations table into a pandas dataframe.\n",
    "\n",
    "    Args:\n",
    "        * file_path (str): The path to the remidations excel file\n",
    "\n",
    "    Returns:\n",
    "        A pandas `DataFrame` object with the remediations data\n",
    "    \"\"\"\n",
    "    \n",
    "    # the row to use as column names\n",
    "    header=2\n",
    "\n",
    "    # for multiindex i.e the file has merged cells\n",
    "    index_col=[0,1,2,3,4]\n",
    "\n",
    "    # renamming the columns\n",
    "    cols=[\n",
    "        \"threat_id\",\n",
    "        \"threat_desc\",\n",
    "        \"vuln_id\",\n",
    "        \"vuln_desc\",\n",
    "        \"vthe\",\n",
    "        \"countermeasure_id\",\n",
    "        \"countermeasure_desc\",\n",
    "        \"tech_nature\"\n",
    "    ]\n",
    "\n",
    "    df = pd.read_excel(\n",
    "        file_path,\n",
    "        header=header,\n",
    "        index_col=index_col\n",
    "    )\n",
    "\n",
    "    # resetting the index as some cells are merged and \n",
    "    # result in `NaN` values\n",
    "    df = df.reset_index()\n",
    "\n",
    "    # renamming the columns\n",
    "    df.columns = cols\n",
    "\n",
    "    df = fill_remediation_na(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "def load_scenarios_file(file_path: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Loads the scenarios table into a pandas dataframe.\n",
    "\n",
    "    Args:\n",
    "        * file_path (str): The path to the scenarios excel file\n",
    "\n",
    "    Returns:\n",
    "        A pandas `DataFrame` object with the scenarios data\n",
    "    \"\"\"\n",
    "\n",
    "    # renamming the columns\n",
    "    cols = [\n",
    "        \"scenario_id\",\n",
    "        \"scenario_desc\",\n",
    "        \"extended\",\n",
    "        \"short\",\n",
    "        \"details\",\n",
    "        \"risk_id\",\n",
    "        \"risk_desc\",\n",
    "        \"vuln_id\",\n",
    "        \"vuln_desc\",\n",
    "        \"risk_occur_type\"\n",
    "    ]\n",
    "\n",
    "    df = pd.read_excel(file_path)\n",
    "\n",
    "    # renamming the columns\n",
    "    df.columns = cols\n",
    "\n",
    "    return df\n",
    "\n",
    "def fill_remediation_na(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Fills the `NaN` values in the remediations data frame.\n",
    "    The remediation data loaded into a `DataFrame` has \n",
    "    some `threat_id` values that are not filled and are `NaN`. This method attempts to fix it\n",
    "    by:\n",
    "        * Identifying the the `NaN` `index`\n",
    "        * determining the value at `index-1`\n",
    "        * filling subsequent indices with value\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): `DataFrame` with the remediations data\n",
    "\n",
    "    Returns:\n",
    "        `DataFrame` with `NaN` values replaced by appropriate threat ids\n",
    "    \"\"\"\n",
    "    \n",
    "    nan_indices = df[df[\"threat_id\"].isna()].index\n",
    "    start = nan_indices[0]\n",
    "    val = df.loc[start-1][\"threat_id\"]\n",
    "\n",
    "    for i in range(0, len(nan_indices)):\n",
    "        if nan_indices[i] != nan_indices[i-1]+1:\n",
    "            # stop filling and change start index\n",
    "            val = df.loc[nan_indices[i]-1][\"threat_id\"]\n",
    "        # otherwise keep filling values\n",
    "        df.loc[nan_indices[i], \"threat_id\"] = val\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ab6f01-3633-4e3d-8254-c08342343fc9",
   "metadata": {},
   "source": [
    "## Data Formatting\n",
    "\n",
    "Methods used for formatting and converting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e3b64f-5e00-464c-aca4-8eb52326b8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RemediationData(BaseModel):\n",
    "    scenario_id: str = Field(description=\"The scenario ID\")\n",
    "    threat_id: str = Field(description=\"The threat id\")\n",
    "    vuln_id: str = Field(description=\"The vulnerability ID\")\n",
    "    remediation_ids: list = Field(description=\"The ID's of the most appropriate countermeasures\")\n",
    "    classification_desc: str = Field(description=\"The description of classification\")\n",
    "    reasoning: list[str] = Field(description=\"Your detailed reasoning behind your suggestion of each countermeasure individually.\")\n",
    "\n",
    "class CustomJsonOutputParser(JsonOutputParser):\n",
    "\n",
    "    def parse_result(self, result: List[Generation], *, partial: bool = False) -> Any:\n",
    "        \"\"\"Parse the result of an LLM call to a JSON object.\n",
    "\n",
    "        Args:\n",
    "            result: The result of the LLM call.\n",
    "            partial: Whether to parse partial JSON objects.\n",
    "                If True, the output will be a JSON object containing\n",
    "                all the keys that have been returned so far.\n",
    "                If False, the output will be the full JSON object.\n",
    "                Default is False.\n",
    "\n",
    "        Returns:\n",
    "            The parsed JSON object.\n",
    "\n",
    "        Raises:\n",
    "            OutputParserException: If the output is not valid JSON.\n",
    "        \"\"\"\n",
    "        text = result[0].text\n",
    "        text = text.strip()\n",
    "\n",
    "        try:\n",
    "            # extracting the model thought and output\n",
    "            match = re.search(r\"<Thought>\\s*(.*?)\\s*</Thought>.*?<Output>\\s*(\\{.*?\\})\\s*</Output>\", text, re.DOTALL)\n",
    "        \n",
    "            group_1 = match.group(1)\n",
    "            group_2 = match.group(2)\n",
    "        \n",
    "            output = json.loads(group_2)\n",
    "            output.update({\"thought\": group_1})\n",
    "            return output\n",
    "            \n",
    "        except JSONDecodeError as e:\n",
    "            msg = f\"Invalid json output: {text}\"\n",
    "            raise OutputParserException(msg, llm_output=text) from e\n",
    "\n",
    "def convert_df_to_dict(df: DataFrame, save_path=None) -> dict:\n",
    "    \"\"\"\n",
    "    Converts a pandas `DataFrame` to python dictionary format\n",
    "\n",
    "    Args:\n",
    "        * df (DataFrame): A panads `DataFrame` object with required data\n",
    "        * save_path (str): path if want to save the json (dict) data in a file\n",
    "    Returns:\n",
    "        data in `dict` format\n",
    "    \"\"\"\n",
    "\n",
    "    json_data = df.to_json(orient=\"records\")\n",
    "    dict_data = json.loads(json_data)\n",
    "\n",
    "    if save_path:\n",
    "        with open(save_path, 'w') as f:\n",
    "            json.dump(dict_data, f)\n",
    "            \n",
    "    return dict_data\n",
    "\n",
    "def convert_llm_json_output_to_csv(llm_output: list[dict], file_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Takes the llms output as json and converts to csv.\n",
    "    The method saves multiple json dicts into a csv file.\n",
    "\n",
    "    Args:\n",
    "        llm_output (list[dict]): `list` of json entries of multiple scenario outputs\n",
    "        file_path (str): name of the csv file where the data will be saved\n",
    "    \"\"\"\n",
    "\n",
    "    with open(file_path, 'w') as outfile:\n",
    "        csv_writer = csv.writer(outfile)\n",
    "        header = outputs[0].keys()\n",
    "        csv_writer.writerow(header)\n",
    "        for data in outputs:\n",
    "            rows = []\n",
    "            # there could be multiple remediations\n",
    "            # we want each seperetly in its own row\n",
    "            for remediation, reason in zip(data['remediation_ids'], data['reasoning']):\n",
    "                row = {\n",
    "                    'scenario_id': data['scenario_id'],\n",
    "                    'threat_id': data['threat_id'],\n",
    "                    'vuln_id': data['vuln_id'],\n",
    "                    'remediation_id': remediation,\n",
    "                    'classification_desc': data['classification_desc'],\n",
    "                    'reasoning': reason,\n",
    "                    'thought': data['thought']\n",
    "                }\n",
    "                rows.append(row)\n",
    "            for row in rows:\n",
    "                csv_writer.writerow(row.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f44ffa-df7a-495b-907b-23b1f179ffa2",
   "metadata": {},
   "source": [
    "## RAG Functions\n",
    "\n",
    "These are methods required to setup the RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07463ee8-2247-41c8-b04f-5f953c987305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_prompt(template, parser) -> ChatPromptTemplate:\n",
    "    \"\"\"\n",
    "    Set up the chat prompt to be used with the model\n",
    "\n",
    "    Args:\n",
    "        * template (str): The prompt template to use\n",
    "        * parser: the parser to use\n",
    "    Returns:\n",
    "        `ChatPromptTemplate` object\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        template=template,\n",
    "        input_variables=[\"user\", \"scenarios\", \"remediations\"],\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "def load_lrm_model_from_hf(model_id) -> ChatHuggingFace:\n",
    "    \"\"\"\n",
    "    Loads lrm model from hugging face to a `ChatHuggingFace` model\n",
    "\n",
    "    Args:\n",
    "        model_id (str): the hugging face url of the model\n",
    "\n",
    "    Returns:\n",
    "        A `ChatHuggingFace` model\n",
    "    \"\"\"\n",
    "\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "    )\n",
    "\n",
    "    llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=model_id,\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs=dict(\n",
    "        max_new_tokens=2048,\n",
    "        do_sample=False,\n",
    "        repetition_penalty=1.03,\n",
    "        return_full_text=False\n",
    "        ),\n",
    "    model_kwargs={\"quantization_config\": quantization_config},\n",
    "    device_map=\"auto\",\n",
    "    )\n",
    "    model = ChatHuggingFace(llm=llm)\n",
    "    return model\n",
    "\n",
    "def create_retrievar_from_vector_store(docs: list):\n",
    "    \"\"\"\n",
    "    Create a retrievar from a vector store.\n",
    "    Embeds documnents, stores into a vector store and creates\n",
    "    a retrievar that can be used to retrieve relevant documents.\n",
    "\n",
    "    Args:\n",
    "        docs (list): A list of `Documents` which need to be embeded.\n",
    "\n",
    "    Returns:\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    embed = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    vector_store = DocArrayInMemorySearch.from_documents(docs, embed)\n",
    "    retrievar = vector_store.as_retriever()\n",
    "\n",
    "    return retrievar\n",
    "\n",
    "def remediation_lookup(scenario_docs: list[Document]) -> dict:\n",
    "    \"\"\"\n",
    "    Get the list of remediations knowing the threat and\n",
    "    vulnerability\n",
    "    The method performs a lookup to get possible countermeasures\n",
    "    to provided threat and vulnerability.\n",
    "\n",
    "    Args:\n",
    "\n",
    "    Returns:\n",
    "        dictionary of possible countermeasures\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: handle NoneType\n",
    "    # TODO: handle leading spaces in doc.page_content\n",
    "    countermeasures = []\n",
    "    remediations_df = load_remediations_file(\"./data/Remediations.xlsx\")\n",
    "\n",
    "    for doc in scenario_docs:\n",
    "        \n",
    "        scen_dict = json.loads(doc.page_content)\n",
    "        if scen_dict[\"risk_id\"] is None:\n",
    "            continue\n",
    "        threat_id = scen_dict[\"risk_id\"].strip()\n",
    "        vuln_id = scen_dict[\"vuln_id\"].strip()\n",
    "        x = remediations_df[remediations_df[\"threat_id\"]==threat_id]\n",
    "        df = x[x[\"vuln_id\"]==vuln_id]\n",
    "        \n",
    "        countermeasures.extend(convert_df_to_dict(df))\n",
    "    \n",
    "    return countermeasures\n",
    "\n",
    "def setup_rag_chain(model_id: str, template: str) -> RunnableSequence:\n",
    "    \"\"\"\n",
    "    Create the rag chain.\n",
    "    Creates the chain to be used for the RAG process\n",
    "\n",
    "    Args:\n",
    "        model_id (str): the name of the lrm model to use\n",
    "        template (str): the template to provide to the lrm. Needs to have\n",
    "                        the keys `(user, scenarios, countermeasures)`\n",
    "\n",
    "    Returns:\n",
    "        a `RunnableSequence` chain that implements the RAG workflow\n",
    "    \"\"\"\n",
    "    scenarios_df = load_scenarios_file(\"./data/Scenarios.xlsx\")\n",
    "    scenarios_dict = convert_df_to_dict(scenarios_df, save_path=\"./data/scen.json\")\n",
    "    loader_scen = JSONLoader(file_path=\"./data/scen.json\",jq_schema='.[]', text_content=False)\n",
    "    scenarios_doc = loader_scen.load()\n",
    "    scenarios = create_retrievar_from_vector_store(scenarios_doc)\n",
    "\n",
    "    json_parser = CustomJsonOutputParser(pydantic_object=RemediationData)\n",
    "    \n",
    "    prompt = set_prompt(TEMPLATE, parser=json_parser)\n",
    "    \n",
    "    model = load_lrm_model_from_hf(model_id=model_id)\n",
    "\n",
    "    \n",
    "    output_parser = StrOutputParser()\n",
    "\n",
    "    chain = (\n",
    "        {\n",
    "            \"user\": RunnablePassthrough(),\n",
    "            \"scenarios\": scenarios,\n",
    "            \"countermeasures\": scenarios | RunnableLambda(remediation_lookup),\n",
    "        } \n",
    "        | prompt\n",
    "        | model\n",
    "        | json_parser        \n",
    "    )\n",
    "\n",
    "    return chain\n",
    "\n",
    "def execute_scenarios(scenarios: list[str], chain) -> list[str]:\n",
    "    \"\"\"\n",
    "    Prompt the llm for multiple user scenarios.\n",
    "\n",
    "    Args:\n",
    "        scenarios (list[str]): list of multiple user scenarios\n",
    "        chain: the rag chain\n",
    "\n",
    "    Returns:\n",
    "        the llm output as a list of strings for each user scenario\n",
    "    \"\"\"\n",
    "\n",
    "    outputs = []\n",
    "    \n",
    "    for scen in scenarios:\n",
    "        out = chain.invoke(scen)\n",
    "        outputs.append(out)\n",
    "\n",
    "    return outputs\n",
    "\n",
    "def update_chain_prompt_template(chain: RunnableSequence, parser=CustomJsonOutputParser(pydantic_object=RemediationData), template: str = TEMPLATE ) -> RunnableSequence:\n",
    "    \"\"\"\n",
    "    Update the prompt template for the chain sequence without having to create\n",
    "    the chain again.\n",
    "\n",
    "    Args:\n",
    "        chain (RunnableSequence): The chain which needs to be updated\n",
    "        parser: the parser to use for the template. Defaults to the `CustomJsonOutputParser`\n",
    "        template (str): the new prompt template. Defaults to `TEMPLATE`\n",
    "    Returns:\n",
    "        the updated chain\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = set_prompt(TEMPLATE_RAG, parser)\n",
    "    chain.assign(prompt=prompt)\n",
    "    return chain\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a975646f-3cdc-4949-909a-6c7a82ed97aa",
   "metadata": {},
   "source": [
    "## RAG Workflow\n",
    "\n",
    "This is where the RAG workflow begins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b0a280-8ecf-466e-8689-45d089f1284c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define user scenarios\n",
    "\n",
    "scenarios = [\n",
    "    \"Only authorized users can open the cabinets containing classified documents and no tracking is required.\",\n",
    "    \"Computers and servers are delivered to the equipment maintenance technicians for repair with the mass storage media inserted.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240d83b7-7814-4983-9c22-5c693fa9758e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the RAG chain\n",
    "\n",
    "chain = setup_rag_chain(model_id=\"O1-OPEN/OpenO1-LLama-8B-v0.1\", template=TEMPLATE_RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fabdc61-049e-4230-bcb5-463cd9b553f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"You can use this method to update the prompt for the chain\n",
    "    without having to create the chain again\n",
    "\"\"\"\n",
    "\n",
    "# csv_parser = CustomCSVParser()\n",
    "# chain = update_chain_prompt_template(chain, csv_parser, NEW_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035069ef-30dd-44ca-8035-2a412a7f8c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute all user scnearios\n",
    "outputs = execute_scenarios(scenarios, chain)\n",
    "\n",
    "\n",
    "# or invoke one your self\n",
    "# output = chain.invoke(\"Computers and servers are delivered to the equipment maintenance technicians for repair with the mass storage media inserted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288f3744-8199-4a5a-a903-fca54e721378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the outputs to human readable csv\n",
    "convert_llm_json_output_to_csv(outputs, \"./data/outputs.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
