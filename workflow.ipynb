{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ead90c61-6d05-4745-975c-dc61d16e4f3d",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb70618-c260-4633-9e43-1de609305f60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install langchain langchain_core langchain_community langchain-huggingface torch accelerate bitsandbytes docarray unstructured jq openpyxl matplotlib numpy pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b74d267-a2ea-48b5-b2d3-cf5df55d918a",
   "metadata": {},
   "source": [
    "## Environment Variables and Constants\n",
    "\n",
    "Set the API keys and environment variables required for running the app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33efd142-6c55-40ea-b734-e99f11085006",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"\"\n",
    "os.environ['LANGCHAIN_API_KEY'] = \"\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "\n",
    "# this defines the location where the models will be downloaded\n",
    "# it is suggested to set this to a location with sufficient storage\n",
    "# space. Not specifying it will default to `${HOME}/.cache/hugging_face`\n",
    "os.environ[\"HF_HUB_CACHE\"] = \"\"\n",
    "os.environ[\"HF_HOME\"] = \"\"\n",
    "\n",
    "TEMPLATE = \"\"\"\n",
    "You are an assistant in security risk analysis.\n",
    "You will be provided with risk scenarios that have certain threats and vulnerabilities. For the threats you will also be provided with possible counter measures.\n",
    "You will be provided with a user scenario and based on that you will be provided with context of related scenarios from you retrieval vector store.\n",
    "You will also be provided with possible countermeasures for all the retrieved scenarios.\n",
    "You need to suggest the appropriate countermeasures for the user scenario and give reasoning as to why it is appropriate. There could be multiple appropriate countermeasures so you need to provide a reasoning for each individually.\n",
    "Answer the question based only on the following context. If the question does not relate with the context, just reply 'I don't know'\n",
    "\n",
    "User: {user}\n",
    "\n",
    "Scenarios: {scenarios}\n",
    "\n",
    "Countermeasures: {countermeasures}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c92f463-8c24-46cd-9b32-ca5a098efd6b",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "Install the requirements and import relevant modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7120a69-f4f9-427c-a109-174941a2ec8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "from json import JSONDecodeError\n",
    "from typing import Any, List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from langchain_community.document_loaders import (JSONLoader,\n",
    "                                                  UnstructuredExcelLoader)\n",
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "from langchain_core.documents.base import Document\n",
    "from langchain_core.exceptions import OutputParserException\n",
    "from langchain_core.output_parsers import JsonOutputParser, StrOutputParser\n",
    "from langchain_core.outputs import Generation\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.runnables.base import RunnableLambda, RunnableSequence\n",
    "from langchain_huggingface import (ChatHuggingFace, HuggingFaceEmbeddings,\n",
    "                                   HuggingFacePipeline)\n",
    "from pandas import DataFrame\n",
    "from transformers import BitsAndBytesConfig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58faceb8-cf6a-4da4-8510-b33d55bdebf9",
   "metadata": {},
   "source": [
    "## Data Loaders\n",
    "\n",
    "Methods used for loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3479ee1-2c7c-439a-85e4-037a4a2518a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_remediations_file(file_path: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Loads the remediations table into a pandas dataframe.\n",
    "\n",
    "    Args:\n",
    "        * file_path (str): The path to the remidations excel file\n",
    "\n",
    "    Returns:\n",
    "        A pandas `DataFrame` object with the remediations data\n",
    "    \"\"\"\n",
    "    \n",
    "    # the row to use as column names\n",
    "    header=2\n",
    "\n",
    "    # for multiindex i.e the file has merged cells\n",
    "    index_col=[0,1,2,3,4]\n",
    "\n",
    "    # renamming the columns\n",
    "    cols=[\n",
    "        \"threat_id\",\n",
    "        \"threat_desc\",\n",
    "        \"vuln_id\",\n",
    "        \"vuln_desc\",\n",
    "        \"vthe\",\n",
    "        \"countermeasure_id\",\n",
    "        \"countermeasure_desc\",\n",
    "        \"tech_nature\"\n",
    "    ]\n",
    "\n",
    "    df = pd.read_excel(\n",
    "        file_path,\n",
    "        header=header,\n",
    "        index_col=index_col\n",
    "    )\n",
    "\n",
    "    # resetting the index as some cells are merged and \n",
    "    # result in `NaN` values\n",
    "    df = df.reset_index()\n",
    "\n",
    "    # renamming the columns\n",
    "    df.columns = cols\n",
    "\n",
    "    df = fill_remediation_na(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "def load_scenarios_file(file_path: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Loads the scenarios table into a pandas dataframe.\n",
    "\n",
    "    Args:\n",
    "        * file_path (str): The path to the scenarios excel file\n",
    "\n",
    "    Returns:\n",
    "        A pandas `DataFrame` object with the scenarios data\n",
    "    \"\"\"\n",
    "\n",
    "    # renamming the columns\n",
    "    cols = [\n",
    "        \"scenario_id\",\n",
    "        \"scenario_desc\",\n",
    "        \"extended\",\n",
    "        \"short\",\n",
    "        \"details\",\n",
    "        \"risk_id\",\n",
    "        \"risk_desc\",\n",
    "        \"vuln_id\",\n",
    "        \"vuln_desc\",\n",
    "        \"risk_occur_type\"\n",
    "    ]\n",
    "\n",
    "    df = pd.read_excel(file_path)\n",
    "\n",
    "    # renamming the columns\n",
    "    df.columns = cols\n",
    "\n",
    "    return df\n",
    "\n",
    "def fill_remediation_na(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Fills the `NaN` values in the remediations data frame.\n",
    "    The remediation data loaded into a `DataFrame` has \n",
    "    some `threat_id` values that are not filled and are `NaN`. This method attempts to fix it\n",
    "    by:\n",
    "        * Identifying the the `NaN` `index`\n",
    "        * determining the value at `index-1`\n",
    "        * filling subsequent indices with value\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): `DataFrame` with the remediations data\n",
    "\n",
    "    Returns:\n",
    "        `DataFrame` with `NaN` values replaced by appropriate threat ids\n",
    "    \"\"\"\n",
    "    \n",
    "    nan_indices = df[df[\"threat_id\"].isna()].index\n",
    "    start = nan_indices[0]\n",
    "    val = df.loc[start-1][\"threat_id\"]\n",
    "\n",
    "    for i in range(0, len(nan_indices)):\n",
    "        if nan_indices[i] != nan_indices[i-1]+1:\n",
    "            # stop filling and change start index\n",
    "            val = df.loc[nan_indices[i]-1][\"threat_id\"]\n",
    "        # otherwise keep filling values\n",
    "        df.loc[nan_indices[i], \"threat_id\"] = val\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ab6f01-3633-4e3d-8254-c08342343fc9",
   "metadata": {},
   "source": [
    "## Data Formatting\n",
    "\n",
    "Methods used for formatting and converting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e3b64f-5e00-464c-aca4-8eb52326b8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RemediationData(BaseModel):\n",
    "    scenario_id: str = Field(description=\"The scenario ID\")\n",
    "    threat_id: str = Field(description=\"The threat id\")\n",
    "    vuln_id: str = Field(description=\"The vulnerability ID\")\n",
    "    remediation_ids: list = Field(description=\"The ID's of the most appropriate countermeasures.\")\n",
    "    classification_desc: str = Field(description=\"The description of classification\")\n",
    "    reasoning: list[str] = Field(description=\"Your reasoning behind your suggestion of each countermeasure.\")\n",
    "\n",
    "class CustomJsonOutputParser(JsonOutputParser):\n",
    "\n",
    "    def parse_result(self, result: List[Generation], *, partial: bool = False) -> Any:\n",
    "        \"\"\"Parse the result of an LLM call to a JSON object.\n",
    "\n",
    "        Args:\n",
    "            result: The result of the LLM call.\n",
    "            partial: Whether to parse partial JSON objects.\n",
    "                If True, the output will be a JSON object containing\n",
    "                all the keys that have been returned so far.\n",
    "                If False, the output will be the full JSON object.\n",
    "                Default is False.\n",
    "\n",
    "        Returns:\n",
    "            The parsed JSON object.\n",
    "\n",
    "        Raises:\n",
    "            OutputParserException: If the output is not valid JSON.\n",
    "        \"\"\"\n",
    "\n",
    "        text = result[0].text\n",
    "        text = text.strip()\n",
    "\n",
    "        try:\n",
    "            # extracting the model thought and output\n",
    "            match = re.search(r\"<Thought>\\s*(.*?)\\s*</Thought>.*?<Output>\\s*(\\{.*?\\})\\s*</Output>\", text, re.DOTALL)\n",
    "        \n",
    "            group_1 = match.group(1)\n",
    "            group_2 = match.group(2)\n",
    "        \n",
    "            output = json.loads(group_2)\n",
    "            output.update({\"thought\": group_1})\n",
    "            return output\n",
    "            \n",
    "        except JSONDecodeError as e:\n",
    "            msg = f\"Invalid json output: {text}\"\n",
    "            try:\n",
    "                raise OutputParserException(msg, llm_output=text) from e\n",
    "            except OutputParserException as e:\n",
    "                print(e)\n",
    "        except:\n",
    "            print(\"Error Occured\")\n",
    "\n",
    "def convert_df_to_dict(df: DataFrame, save_path=None) -> dict:\n",
    "    \"\"\"\n",
    "    Converts a pandas `DataFrame` to python dictionary format\n",
    "\n",
    "    Args:\n",
    "        * df (DataFrame): A panads `DataFrame` object with required data\n",
    "        * save_path (str): path if want to save the json (dict) data in a file\n",
    "    Returns:\n",
    "        data in `dict` format\n",
    "    \"\"\"\n",
    "\n",
    "    json_data = df.to_json(orient=\"records\")\n",
    "    dict_data = json.loads(json_data)\n",
    "\n",
    "    if save_path:\n",
    "        with open(save_path, 'w') as f:\n",
    "            json.dump(dict_data, f)\n",
    "            \n",
    "    return dict_data\n",
    "\n",
    "def convert_llm_json_output_to_csv(llm_output: list[dict], file_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Takes the llms output as json and converts to csv.\n",
    "    The method saves multiple json dicts into a csv file.\n",
    "\n",
    "    Args:\n",
    "        llm_output (list[dict]): `list` of json entries of multiple scenario outputs\n",
    "        file_path (str): name of the csv file where the data will be saved\n",
    "    \"\"\"\n",
    "\n",
    "    with open(file_path, 'w') as outfile:\n",
    "        csv_writer = csv.writer(outfile)\n",
    "        header = outputs[1].keys()\n",
    "        csv_writer.writerow(header)\n",
    "        for index, data in enumerate(outputs):\n",
    "            rows = []\n",
    "            # there could be multiple remediations\n",
    "            # we want each seperetly in its own row\n",
    "            try:\n",
    "                for remediation, reason in zip(data['remediation_ids'], data['reasoning']):\n",
    "                    row = {\n",
    "                        'scenario_id': data['scenario_id'],\n",
    "                        'threat_id': data['threat_id'],\n",
    "                        'vuln_id': data['vuln_id'],\n",
    "                        'remediation_id': remediation,\n",
    "                        'classification_desc': data['classification_desc'],\n",
    "                        'reasoning': reason,\n",
    "                        'thought': data['thought']\n",
    "                    }\n",
    "                    rows.append(row)\n",
    "            except:\n",
    "                print(f\"Error occured at index{index}\")\n",
    "                continue\n",
    "            for row in rows:\n",
    "                csv_writer.writerow(row.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f44ffa-df7a-495b-907b-23b1f179ffa2",
   "metadata": {},
   "source": [
    "## RAG Functions\n",
    "\n",
    "These are methods required to setup the RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07463ee8-2247-41c8-b04f-5f953c987305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_prompt(template, parser) -> ChatPromptTemplate:\n",
    "    \"\"\"\n",
    "    Set up the chat prompt to be used with the model\n",
    "\n",
    "    Args:\n",
    "        * template (str): The prompt template to use\n",
    "        * parser: the parser to use\n",
    "    Returns:\n",
    "        `ChatPromptTemplate` object\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        template=template,\n",
    "        input_variables=[\"user\", \"scenarios\", \"remediations\"],\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "def load_lrm_model_from_hf(model_id) -> ChatHuggingFace:\n",
    "    \"\"\"\n",
    "    Loads lrm model from hugging face to a `ChatHuggingFace` model\n",
    "\n",
    "    Args:\n",
    "        model_id (str): the hugging face url of the model\n",
    "\n",
    "    Returns:\n",
    "        A `ChatHuggingFace` model\n",
    "    \"\"\"\n",
    "\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "    )\n",
    "\n",
    "    llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=model_id,\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs=dict(\n",
    "        max_new_tokens=10000,\n",
    "        do_sample=False,\n",
    "        repetition_penalty=1.03,\n",
    "        return_full_text=False\n",
    "        ),\n",
    "    model_kwargs={\"quantization_config\": quantization_config},\n",
    "    device_map=\"auto\",\n",
    "    )\n",
    "    model = ChatHuggingFace(llm=llm)\n",
    "    return model\n",
    "\n",
    "def create_retrievar_from_vector_store(docs: list):\n",
    "    \"\"\"\n",
    "    Create a retrievar from a vector store.\n",
    "    Embeds documnents, stores into a vector store and creates\n",
    "    a retrievar that can be used to retrieve relevant documents.\n",
    "\n",
    "    Args:\n",
    "        docs (list): A list of `Documents` which need to be embeded.\n",
    "\n",
    "    Returns:\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    embed = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    vector_store = DocArrayInMemorySearch.from_documents(docs, embed)\n",
    "    retrievar = vector_store.as_retriever()\n",
    "\n",
    "    return retrievar\n",
    "\n",
    "def remediation_lookup(scenario_docs: list[Document]) -> dict:\n",
    "    \"\"\"\n",
    "    Get the list of remediations knowing the threat and\n",
    "    vulnerability\n",
    "    The method performs a lookup to get possible countermeasures\n",
    "    to provided threat and vulnerability.\n",
    "\n",
    "    Args:\n",
    "\n",
    "    Returns:\n",
    "        dictionary of possible countermeasures\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: handle NoneType\n",
    "    # TODO: handle leading spaces in doc.page_content\n",
    "    countermeasures = []\n",
    "    remediations_df = load_remediations_file(\"./data/Remediations.xlsx\")\n",
    "\n",
    "    for doc in scenario_docs:\n",
    "        \n",
    "        scen_dict = json.loads(doc.page_content)\n",
    "        if scen_dict[\"risk_id\"] is None:\n",
    "            continue\n",
    "        threat_id = scen_dict[\"risk_id\"].strip()\n",
    "        vuln_id = scen_dict[\"vuln_id\"].strip()\n",
    "        x = remediations_df[remediations_df[\"threat_id\"]==threat_id]\n",
    "        df = x[x[\"vuln_id\"]==vuln_id]\n",
    "        \n",
    "        countermeasures.extend(convert_df_to_dict(df))\n",
    "    \n",
    "    return countermeasures\n",
    "\n",
    "def setup_rag_chain(model_id: str, template: str) -> RunnableSequence:\n",
    "    \"\"\"\n",
    "    Create the rag chain.\n",
    "    Creates the chain to be used for the RAG process\n",
    "\n",
    "    Args:\n",
    "        model_id (str): the name of the lrm model to use\n",
    "        template (str): the template to provide to the lrm. Needs to have\n",
    "                        the keys `(user, scenarios, countermeasures)`\n",
    "\n",
    "    Returns:\n",
    "        a `RunnableSequence` chain that implements the RAG workflow\n",
    "    \"\"\"\n",
    "    scenarios_df = load_scenarios_file(\"./data/Scenarios.xlsx\")\n",
    "    scenarios_dict = convert_df_to_dict(scenarios_df, save_path=\"./data/scen.json\")\n",
    "    loader_scen = JSONLoader(file_path=\"./data/scen.json\",jq_schema='.[]', text_content=False)\n",
    "    scenarios_doc = loader_scen.load()\n",
    "    scenarios = create_retrievar_from_vector_store(scenarios_doc)\n",
    "\n",
    "    json_parser = CustomJsonOutputParser(pydantic_object=RemediationData)\n",
    "    \n",
    "    prompt = set_prompt(TEMPLATE, parser=json_parser)\n",
    "    \n",
    "    model = load_lrm_model_from_hf(model_id=model_id)\n",
    "\n",
    "    \n",
    "    output_parser = StrOutputParser()\n",
    "\n",
    "    chain = (\n",
    "        {\n",
    "            \"user\": RunnablePassthrough(),\n",
    "            \"scenarios\": scenarios,\n",
    "            \"countermeasures\": scenarios | RunnableLambda(remediation_lookup),\n",
    "        } \n",
    "        | prompt\n",
    "        | model\n",
    "        | json_parser        \n",
    "    )\n",
    "\n",
    "    return chain\n",
    "\n",
    "def execute_scenarios(scenarios: list[str], chain) -> list[str]:\n",
    "    \"\"\"\n",
    "    Prompt the llm for multiple user scenarios.\n",
    "\n",
    "    Args:\n",
    "        scenarios (list[str]): list of multiple user scenarios\n",
    "        chain: the rag chain\n",
    "\n",
    "    Returns:\n",
    "        the llm output as a list of strings for each user scenario\n",
    "    \"\"\"\n",
    "\n",
    "    outputs = []\n",
    "    for scen in scenarios:\n",
    "        out = chain.invoke(scen)\n",
    "        outputs.append(out)\n",
    "\n",
    "    return outputs\n",
    "\n",
    "def update_chain_prompt_template(chain: RunnableSequence, parser=CustomJsonOutputParser(pydantic_object=RemediationData), template: str = TEMPLATE ) -> RunnableSequence:\n",
    "    \"\"\"\n",
    "    Update the prompt template for the chain sequence without having to create\n",
    "    the chain again.\n",
    "\n",
    "    Args:\n",
    "        chain (RunnableSequence): The chain which needs to be updated\n",
    "        parser: the parser to use for the template. Defaults to the `CustomJsonOutputParser`\n",
    "        template (str): the new prompt template. Defaults to `TEMPLATE`\n",
    "    Returns:\n",
    "        the updated chain\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = set_prompt(template, parser)\n",
    "    chain.assign(prompt=prompt)\n",
    "    return chain\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a975646f-3cdc-4949-909a-6c7a82ed97aa",
   "metadata": {},
   "source": [
    "## RAG Workflow\n",
    "\n",
    "This is where the RAG workflow begins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e903746-bcb0-452a-b349-345c3197707b",
   "metadata": {},
   "source": [
    "First load the user scenario data for input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b0a280-8ecf-466e-8689-45d089f1284c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define user scenarios\n",
    "\n",
    "# scenarios = [\n",
    "#     \"Only authorized users can open the cabinets containing classified documents and no tracking is required.\",\n",
    "#     \"Computers and servers are delivered to the equipment maintenance technicians for repair with the mass storage media inserted.\"\n",
    "# ]\n",
    "\n",
    "scenarios = []\n",
    "with open(\"./data/scen.txt\") as f:\n",
    "    scenarios = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc81b928-bc42-467f-a3eb-160be01b4212",
   "metadata": {},
   "source": [
    "Then create the RAG chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240d83b7-7814-4983-9c22-5c693fa9758e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the RAG chain\n",
    "\n",
    "chain = setup_rag_chain(model_id=\"O1-OPEN/OpenO1-LLama-8B-v0.1\", template=TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fabdc61-049e-4230-bcb5-463cd9b553f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"You can use this method to update the prompt for the chain\n",
    "    without having to create the chain again\n",
    "\"\"\"\n",
    "\n",
    "# csv_parser = CustomCSVParser()\n",
    "# chain = update_chain_prompt_template(chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3ad9a0-6384-4fdd-bbb2-54c4826a65db",
   "metadata": {},
   "source": [
    "Execute the user scenarios and store the json output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035069ef-30dd-44ca-8035-2a412a7f8c81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Execute all user scnearios\n",
    "outputs = execute_scenarios(scenarios, chain)\n",
    "\n",
    "\n",
    "# or invoke one your self\n",
    "# chain.invoke(\"The processing center is located in the basement. A sewer system runs under the building. The walls of the room that houses the processing center are not reinforced.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c9b16b-a155-4f37-b43d-6cea4bdbf6e7",
   "metadata": {},
   "source": [
    "Convert the model json output to a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288f3744-8199-4a5a-a903-fca54e721378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the outputs to human readable csv\n",
    "convert_llm_json_output_to_csv(outputs, \"./data/output.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306ffd6f-7d35-447b-abac-ef686538c691",
   "metadata": {},
   "source": [
    "Also save the output as json for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c11dac3-7a37-4d91-88b6-503ecd69ef0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save the output as json \n",
    "with open(\"./data/output.json\", \"w\") as f:\n",
    "    json.dump(outputs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c362c2-423d-458f-8361-63f17af21312",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce58dd77-b982-4326-80ed-b542a7da51bb",
   "metadata": {},
   "source": [
    "This is the code used for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51736003-5b96-4d04-ac05-184822bbf4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/output.json\") as f:\n",
    "    model_output = json.load(f)\n",
    "with open(\"./data/remediations.json\") as f:\n",
    "    remediation_data = json.load(f)\n",
    "\n",
    "# Loading the data and clearning it\n",
    "df_output = pd.DataFrame([row for row in model_output if isinstance(row, dict)])\n",
    "df_remediations = pd.DataFrame(remediation_data)\n",
    "df_output_clean = df_output.dropna(subset=['scenario_id', 'vuln_id', 'remediation_ids'])\n",
    "df_remediations_clean = df_remediations.dropna(subset=['threat_id', 'vuln_id', 'remediation_id'])\n",
    "\n",
    "# Creating the lookups\n",
    "remediation_lookup = df_remediations_clean.groupby(['threat_id', 'vuln_id'])['remediation_id'].apply(set).to_dict()\n",
    "all_known_remediations = set(df_remediations_clean['remediation_id'].unique())\n",
    "\n",
    "# Variables used for the evaluation\n",
    "tp = fp = fn = 0\n",
    "total_expected = total_covered = 0\n",
    "actionability_scores = []\n",
    "hidden_risks = []\n",
    "false_positives = 0\n",
    "anomaly_types = Counter()\n",
    "case_records = []\n",
    "\n",
    "# This is the main evaluation loop\n",
    "for _, row in df_output_clean.iterrows():\n",
    "    scenario_id = row['scenario_id']\n",
    "    threat_id = row['threat_id']\n",
    "    vuln_id = row['vuln_id']\n",
    "    predicted = set(row['remediation_ids'])\n",
    "\n",
    "    key = (threat_id, vuln_id)\n",
    "    expected = remediation_lookup.get(key, set())\n",
    "    matched = predicted & expected\n",
    "    extras = predicted - expected\n",
    "    missed = expected - predicted\n",
    "\n",
    "    tp += len(matched)\n",
    "    fp += len(extras)\n",
    "    fn += len(missed)\n",
    "    total_expected += len(expected)\n",
    "    total_covered += len(matched)\n",
    "\n",
    "    for remediation in predicted:\n",
    "        if remediation in matched:\n",
    "            score = 5\n",
    "        elif remediation in all_known_remediations:\n",
    "            score = 3\n",
    "        else:\n",
    "            score = 1\n",
    "        actionability_scores.append(score)\n",
    "\n",
    "        case_records.append({\n",
    "            \"scenario_id\": scenario_id,\n",
    "            \"threat_id\": threat_id,\n",
    "            \"vuln_id\": vuln_id,\n",
    "            \"remediation_id\": remediation,\n",
    "            \"actionability\": score,\n",
    "            \"matched\": remediation in expected,\n",
    "            \"hidden_risk\": remediation not in expected and remediation not in all_known_remediations\n",
    "        })\n",
    "\n",
    "    for r in extras:\n",
    "        if r not in all_known_remediations:\n",
    "            hidden_risks.append(r)\n",
    "        else:\n",
    "            false_positives += 1\n",
    "\n",
    "    if any(r not in all_known_remediations for r in extras):\n",
    "        anomaly_types['Hallucination'] += 1\n",
    "    if any(r in all_known_remediations for r in extras):\n",
    "        anomaly_types['Incorrect Prioritization'] += 1\n",
    "    if missed:\n",
    "        anomaly_types['Omission'] += 1\n",
    "\n",
    "# Calculating the metrics\n",
    "precision = tp / (tp + fp) if (tp + fp) else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) else 0\n",
    "f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) else 0\n",
    "comprehensiveness = total_covered / total_expected if total_expected else 0\n",
    "avg_actionability = np.mean(actionability_scores) if actionability_scores else 0\n",
    "valid_hidden = len(set(hidden_risks))\n",
    "hrdr = valid_hidden / (valid_hidden + false_positives) if (valid_hidden + false_positives) else 0\n",
    "\n",
    "# Printing the output summary\n",
    "print(\"Summary:\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Comprehensiveness: {comprehensiveness:.4f}\")\n",
    "print(f\"Avg. Actionability: {avg_actionability:.2f}/5\")\n",
    "print(f\"Hidden Risk Discovery Rate: {hrdr * 100:.2f}%\")\n",
    "print(\"Anomalies:\", dict(anomaly_types))\n",
    "\n",
    "# Preaparing the data for visiualizartions\n",
    "df_case = pd.DataFrame(case_records)\n",
    "df_case['key'] = df_case['threat_id'] + \"_\" + df_case['vuln_id']\n",
    "accuracy_by_case = df_case.groupby('scenario_id')['matched'].mean().reset_index()\n",
    "\n",
    "expected_counts = df_remediations_clean.groupby(['threat_id', 'vuln_id'])['remediation_id'].count().reset_index()\n",
    "expected_counts['key'] = expected_counts['threat_id'] + \"_\" + expected_counts['vuln_id']\n",
    "matched_counts = df_case[df_case['matched']].groupby('key')['remediation_id'].count().reset_index(name='matched')\n",
    "comprehensiveness_df = pd.merge(expected_counts, matched_counts, on='key', how='left').fillna(0)\n",
    "comprehensiveness_df['comprehensiveness'] = comprehensiveness_df['matched'] / comprehensiveness_df['remediation_id']\n",
    "comprehensiveness_df['scenario'] = comprehensiveness_df['key']\n",
    "\n",
    "# Actionability Score Distribution\n",
    "plt.figure(figsize=(8, 5))\n",
    "df_case['actionability'].hist(bins=[1, 2, 3, 4, 5, 6], rwidth=0.9, align='left')\n",
    "plt.title(\"Distribution of Actionability Scores\")\n",
    "plt.xlabel(\"Actionability Score (1–5)\")\n",
    "plt.ylabel(\"Number of Remediation Suggestions\")\n",
    "plt.xticks([1, 2, 3, 4, 5])\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Hidden Risks Per Scenario\n",
    "hidden_risk_counts = df_case[df_case['hidden_risk']].groupby('scenario_id').size()\n",
    "plt.figure(figsize=(10, 5))\n",
    "hidden_risk_counts.plot(kind='bar')\n",
    "plt.title(\"Hidden Risks Identified Per Scenario\")\n",
    "plt.xlabel(\"Scenario ID\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Normalized Metric Overview\n",
    "normalized_metrics = {\n",
    "    \"Precision\": precision,\n",
    "    \"Recall\": recall,\n",
    "    \"F1 Score\": f1,\n",
    "    \"Comprehensiveness\": comprehensiveness,\n",
    "    \"Actionability\": avg_actionability / 5,\n",
    "    \"HRDR\": hrdr\n",
    "}\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(normalized_metrics.keys(), normalized_metrics.values())\n",
    "plt.title(\"Normalized Evaluation Metrics\")\n",
    "plt.ylabel(\"Score (0–1)\")\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Anomaly Breakdown Pie Chart\n",
    "if anomaly_types:\n",
    "    plt.figure(figsize=(7, 7))\n",
    "    plt.pie(anomaly_types.values(), labels=anomaly_types.keys(), autopct='%1.1f%%', startangle=140)\n",
    "    plt.title(\"Anomaly Type Distribution\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
